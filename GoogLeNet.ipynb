{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set split: 75-15-10\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, Dropout\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers.merge import concatenate\n",
    "from tensorflow.python.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_model(input, filters_1x1, filters_3x3_reduce, filters_3x3, \n",
    "                    filters_5x5_reduce, filters_5x5, filters_pool_proj):\n",
    "\n",
    "    conv_1x1 = Conv2D(filters=filters_1x1, kernel_size=(1, 1), padding='same', \n",
    "                      activation='relu', kernel_regularizer=l2(0.01))(input)\n",
    "    \n",
    "    conv_3x3_reduce = Conv2D(filters=filters_3x3_reduce, kernel_size=(1, 1), padding='same', \n",
    "                             activation='relu', kernel_regularizer=l2(0.01))(input)\n",
    "    \n",
    "    conv_3x3 = Conv2D(filters=filters_3x3, kernel_size=(3, 3), padding='same', \n",
    "                      activation='relu', kernel_regularizer=l2(0.01))(conv_3x3_reduce)\n",
    "    \n",
    "    conv_5x5_reduce  = Conv2D(filters=filters_5x5_reduce, kernel_size=(1, 1), padding='same', \n",
    "                              activation='relu', kernel_regularizer=l2(0.01))(input)\n",
    "    \n",
    "    conv_5x5 = Conv2D(filters=filters_5x5, kernel_size=(5, 5), padding='same', \n",
    "                      activation='relu', kernel_regularizer=l2(0.01))(conv_5x5_reduce)\n",
    "    \n",
    "    maxpool = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(input)\n",
    "    \n",
    "    maxpool_proj = Conv2D(filters=filters_pool_proj, kernel_size=(1, 1), strides=(1, 1), padding='same', \n",
    "                          activation='relu', kernel_regularizer=l2(0.01))(maxpool)\n",
    "    \n",
    "    inception_output = concatenate([conv_1x1, conv_3x3, conv_5x5, maxpool_proj], axis=3)  # use tf as backend\n",
    "\n",
    "    return inception_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(weight_path = None):\n",
    "    input = Input(shape=(32, 32, 3))\n",
    "\n",
    "    conv1_7x7_s2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same', \n",
    "                          activation='relu', kernel_regularizer=l2(0.01))(input)\n",
    "\n",
    "    maxpool1_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv1_7x7_s2)\n",
    "\n",
    "    conv2_3x3_reduce = Conv2D(filters=64, kernel_size=(1, 1), padding='same', \n",
    "                              activation='relu', kernel_regularizer=l2(0.01))(maxpool1_3x3_s2)\n",
    "\n",
    "    conv2_3x3 = Conv2D(filters=192, kernel_size=(3, 3), padding='same', \n",
    "                       activation='relu', kernel_regularizer=l2(0.01))(conv2_3x3_reduce)\n",
    "\n",
    "    maxpool2_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv2_3x3)\n",
    "\n",
    "    inception_3a = inception_model(input=maxpool2_3x3_s2, filters_1x1=64, \n",
    "                                   filters_3x3_reduce=96, filters_3x3=128, \n",
    "                                   filters_5x5_reduce=16, filters_5x5=32, filters_pool_proj=32)\n",
    "\n",
    "    inception_3b = inception_model(input=inception_3a, filters_1x1=128, \n",
    "                                   filters_3x3_reduce=128, filters_3x3=192,\n",
    "                                   filters_5x5_reduce=32, filters_5x5=96, filters_pool_proj=64)\n",
    "\n",
    "    maxpool3_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inception_3b)\n",
    "\n",
    "    inception_4a = inception_model(input=maxpool3_3x3_s2, filters_1x1=192, \n",
    "                                   filters_3x3_reduce=96, filters_3x3=208, \n",
    "                                   filters_5x5_reduce=16, filters_5x5=48, filters_pool_proj=64)\n",
    "\n",
    "    inception_4b = inception_model(input=inception_4a, filters_1x1=160,\n",
    "                                   filters_3x3_reduce=112, filters_3x3=224,\n",
    "                                   filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64)\n",
    "\n",
    "    inception_4c = inception_model(input=inception_4b, filters_1x1=128, \n",
    "                                   filters_3x3_reduce=128, filters_3x3=256,\n",
    "                                   filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64)\n",
    "\n",
    "    inception_4d = inception_model(input=inception_4c, filters_1x1=112,\n",
    "                                   filters_3x3_reduce=144, filters_3x3=288, \n",
    "                                   filters_5x5_reduce=32, filters_5x5=64, filters_pool_proj=64)\n",
    "\n",
    "    inception_4e = inception_model(input=inception_4d, filters_1x1=256,\n",
    "                                   filters_3x3_reduce=160, filters_3x3=320,\n",
    "                                   filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=128)\n",
    "\n",
    "    maxpool4_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inception_4e)\n",
    "\n",
    "    inception_5a = inception_model(input=maxpool4_3x3_s2, filters_1x1=256,\n",
    "                                   filters_3x3_reduce=160, filters_3x3=320,\n",
    "                                   filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=128)\n",
    "\n",
    "    inception_5b = inception_model(input=inception_5a, filters_1x1=384,\n",
    "                                   filters_3x3_reduce=192, filters_3x3=384,\n",
    "                                   filters_5x5_reduce=48, filters_5x5=128, filters_pool_proj=128)\n",
    "\n",
    "    averagepool1_7x7_s1 = AveragePooling2D(pool_size=(7, 7), strides=(7, 7), padding='same')(inception_5b)\n",
    "\n",
    "    drop1 = Dropout(rate=0.4)(averagepool1_7x7_s1)\n",
    "\n",
    "    linear = Dense(units=4, activation='softmax', kernel_regularizer=l2(0.01))(Flatten()(drop1))\n",
    "    last = linear\n",
    "\n",
    "\n",
    "    model = Model(inputs=input, outputs=last)\n",
    "    return model\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    model = define_model()\n",
    "    print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=1e-6, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "validation_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('D3/train/', \n",
    "                                                 target_size = (224,224), \n",
    "                                                 batch_size = 10, \n",
    "                                                 class_mode = 'categorical', \n",
    "                                                 shuffle = True)\n",
    "\n",
    "validation_set = test_datagen.flow_from_directory('D3/validation/', \n",
    "                                            target_size = (224,224), \n",
    "                                            batch_size = 1, \n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('D3/test/', \n",
    "                                            target_size = (224,224), \n",
    "                                            batch_size = 1, \n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_3/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = model.fit(training_set, \n",
    "                     epochs = 30, \n",
    "                     validation_data = validation_set, \n",
    "                     verbose = 1, \n",
    "                     callbacks = [cp_callback, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/GoogLeNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model/GoogLeNet')\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model_googlenet.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
